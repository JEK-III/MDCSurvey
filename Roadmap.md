# Roadmap

* Researchers who publish useful data should be acknowledged
* that requires accepted metrics for data impact / value / whatever
* citation counts are the most common for papers, but there are also altmetrics

several efforts underway:
* *RDA bibliometrics WG (https://rd-alliance.org/group/rdawds-publishing-data-bibliometrics-wg/case-statement/rdawds-publishing-data-bibliometrics-wg)
* *NISO Altmetric WG is considering data (http://www.niso.org/topics/tl/altmetrics_initiative/)

Making Data Count: NSF-funded collaboration between CDL, PLOS, DataONE
* work is ongoing
* Goal is to define a set of metrics and develop a tool
* to do that, we surveyed researchers and repo. staff on their needs/interestes WRT metrics

##Methods & Demographics
* *online surveys distributed via social media and listservs

* Researcher demographics:
* *N=247
* *78% academic
* *57% US, 14% UK
* *53% biology, 17% environmental science, 10% social science

* Repo. demographics:
* *N=73
* *64% academic, 22% government
* *72% US, 11% UK


##Locations

###Data sharing
* After a series of interviews with researchers, Kim & Stanton (2012) four main categories: in response to personal requests; via personal, project, or lab website; via upload to an external repository; and as supplemental material published with a journal article.
* 94 (38%) shared most/all of their data in some way.
* 4 (2%) didn't share any data by any of the provided channels.
* Database/repo had the most respondents saying most/all 52 (21%)
* email had the most respondents sharing some (163, 66%); only 22 (9%) hadn't shared any data via email
* *on personal request is the most common method used by researchers (Akers & Doty, 2013; Wallis et al., 2013; Kratz & Strasser, 2015).

###Data discovery
* *63% "definitely" use two or more methods to look for data
* *Use of disciplinary databases, the literature, and general-purpose search engines is roughly similar (59%, 58%, and 51% definite respectively)
* *Social-ish methods are much less well used (but private communications wasn't addressed)


##Metrics & information

###info about data users
* not surprisingly, names & contact info are most interesting to more researchers than anything else (1st choice of 48%), but disciplines actually edge out on average because more researchers ranked names last (13%).
* Geographic location was a distant last place (58% put it last).
* Repos split into two camps: equal numbers (47%) requried users to supply their real names or no information at all.
* Email address and institutional affiliation travel with real name; 


*Quality
* *good documentation is the most important factor (52% #1)
* *surprisingly, reuse is the least important factor (48% #5)
* *consistent with PLOS paper (Kratz and Strasser, 2015)


###Impact
Researcher impact
* *Citations overwhelmingly the first choice (85% #1)
* *Downloads the second (64% %2)
* *Pageviews last at 60% least interesting.
* Citations are the #1 preferred method of receiving credit in (Enke et al., 2012) (71%)
* #1 preferred method of receiving credit in our survey (75%)			

Repo impact
* *citations the most interesting to 61%
* *landing pages the least to 51%

###Repo practices
* Metrics tracked and exposed
* *Downloads the most tracked (85%)
* *Views second (66%)
* *Citations to datasets (the most desireable to researchers) 23%

* Many fewer exposed via API or displayed on item page than tracked
* *30% downloads
* *27% views
* *11% citations



##Practical conclusions
* *citations are the most valuable to everyone
* *repos should expose stats downloads and citations
* *views and links are less important
* *data citation is good, collecting that info is good

* previous survey found citations to be the most useful measure of impact (Kratz & Strasser, 2015)