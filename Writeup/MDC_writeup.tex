\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{babel}
\usepackage[hidelinks]{hyperref}
\begin{document}

\title{Making Data count}


\author{John Kratz\textsuperscript{1}, Carly
Strasser\textsuperscript{2{*}}}

\maketitle
1. California Digital Library 2. DataCite {*}corresponding author(s):
John Kratz (John.Kratz@ucop.edu)


\section*{Comment}

% ==============================================================
% Introduction
% ==============================================================

Data undergirds all of science-- yet scientists, funders, and administrators often fail to recognize data sharing or publication as genuine scholarship.
Among the many prerequisites for such recognition is an accepted set of metrics to gauge the significance of the scientific contribution made by publishing any given dataset.
Journal article impact has long been estimated by counting the number of subsequent articles that cite an article. 
Now, a suite of internet-based alternative metrics (``altmetrics'') seek to provide faster assessment and capture other kinds of impact.
A number of initiatives are underway to develop or adapt metrics suitable for data, including the Research Data Alliance (RDA) Data Bibliometrics Working Group (\url{http://rd-alliance.org/group/rdawds-publishing-data-bibliometrics-wg/case-statement/rdawds-publishing-data-bibliometrics-wg}{}); the National Informations Standards Organization (NISO) Alternative Assessment Metrics Initiative (\url{http://www.niso.org/topics/tl/altmetrics_initiative/}), which explicitly considers altmetrics for non-traditional outputs like software and data; and Making Data Count (\url{http://mdc.plos.org}), a collaboration between the California Digital Library, the Public Library of Science (PLOS), and DataONE to define a suite of Data Level Metrics (DLMs) and adapt the existing PLOS Article Level Metric (ALM) tool(\url{http://alm.plos.org}) to capture them.

% ==============================================================
% Methods and Demographics
% ==============================================================

To define useful DLMs, it is essential to understand the values and needs of the researchers who create and re-use data and the managers (i.e., database and repository staff) who preserve and publish it.
As part of Making Data Count, we asked researchers and data managers questions about data sharing, discovery, and metrics in a pair of online surveys.
We solicited responses via social media, listservs, and posts to CDL and PLOS blogs, ultimately hearing from 247 researchers and 73 data managers in November and December of 2014.
Most researchers worked at academic institutions (78\%) and most were based in the United States (57\%) or United Kingdom (14\%).
Both academic (64\%) and government-run (22\%) repositories were significantly represented; repositories were primarily located in the United States (72\%) or United Kingdom (11\%).
Researchers from across the academic career spectrum responded; principal investigators, postdocs, and grad students were all well represented. 
Biology is the most popular discipline (53\%), but environmental (17\%) and social (10\%) science are also significantly represented. 

% ==============================================================
% Locations for sharing & discovery
% ==============================================================

To situate the collection and display of data metrics, we wanted to know where on the internet researchers go to share their data or to discover other's data. 
Data sharing behavior has been surveyed extensively; consistent with previous surveys\cite{@akers_disciplinary_2013, @wallis_if_2013, @kratz_researcher_2015}, more respondents (90\%) shared at least some data by email than by any other method. 
However, more respondents said they used a database or repository to share most or all of their data (24\%) than email (16\%). 

In contrast, data discovery behavior has not been extensively studied.
A 63\% majority of respondents said that they would ``definitely'' use more than one method to search for data to use. 
Three of the five methods presented would definitely be used by a majority of respondents: searching via the literature, a discipline-specific database, or a general purpose search engine (59\%, 58\%, and 51\% definite respectively). 
When asked to write-in particular sources, more respondents mentioned Dryad (n=16) than any other source; Google and ``journal articles'' were tied for second (n=14). 
Respondents were unlikely to query colleagues via social media or a discussion forum (42\% and 40\% ``no chance'' respectively). 
Email or in-person inquiries to particular colleagues is probably more common, as colleagues were a relatively highly cited write-in source ($n=12$).

% ==============================================================
% Information about data users
% ==============================================================

Not surprisingly, when asked what they would be interested to know about users of their data, more researchers chose ``name and contact information'' as their first choice than any other option.
However, a significant fraction (13\%) ranked this option last, and the average interest in discipline was slightly higher.
The least interesting option we presented was geographic location, put in last place by 58\%.

The current practices of repositories were split roughly in half between repositories that require a real name (47\%), email address (44\%), and institutional affiliation (40\%) and those that provide data without asking for anything (47\%).
Data managers overwhelmingly chose discipline as the most interesting thing to know about users of their data (67\%).

% ==============================================================
% Information about data impact
% ==============================================================

Citations are overwhelmingly valued as the coin of scholarly prestige. 
When asked to rank potential information about data use, 85\% of researchers and 61\% of data managers chose citations as the most interesting thing to know about their data. 
The preference of researchers is consistent with previous surveys\cite{@kratz_researcher_2015}, but it is more surprising that data managers prefer a purely scholarly measure so clearly over ``real world'' impact (ranked first by only 23\%). 
Download count was a consistent second choice of researchers (by 64\%). 
Landing page views were ranked last by >50\% of both researchers and data managers.

As a practical matter, it is important to understand what metrics are already being tracked and exposed by repositories. 
We found that most repositories track downloads (85\%) and landing page views (66\%). 
However, only 35\% of the repositories that track downloads expose them via API or display on the landing page; this ratio is roughly similar for all of the metrics we asked about. 
Despite the extreme interest in citation counts, relatively few repositories (23\%) track them. 

% ==============================================================
% Conclusions & future work 
% ==============================================================

Citation counts are the gold standard, but there  are two major obstacles to citation counts: one is that data is often not cited formally; the other is that the infrastructure to track data citations isn't entirely in place.
The scholarly communication community recognized the consensus that data used in an article should be cited formally in the reference list with the Joint Declaration of Data Citation Principles (\url{http://www.force11.org/node/4769}). 
Researchers agree: 95\% of respondents to a DataONE survey agreed that formal citation was a fair condition for data sharing, as did 87\% of astrobiologists in a follow-up survey \cite{@tenopir_data_2011, @aydinoglu_data_2014}. 
Citation``in the references like normal publications'' is the preferred method of receiving credit for data sharing by 71\% of biodiversity researchers and by 75\% of respondents to our earlier survey \cite{@enke_users_2012, @kratz_researcher_2015}.
The Thomson-Reuters Data Citation Index (\url{http://thomsonreuters.com/data-citation-index/}) is a good start, but we need to firm up this infrastructure.


\section*{Acknowledgements}



Text acknowledging non-author contributors. Acknowledgements should
be brief, and should not include thanks to anonymous referees and
editors, or effusive comments. Grant or contribution numbers may be
acknowledged. Author contributions Please describe briefly the contributions
of each author to this work on a separate line. 

Making Data Count is funded by National Science Foundation (NSF) grant number 1448821.

AK did this and that. 

BG did this and that and the other. 


\section*{Competing financial interests}


The author(s) declare no competing financial interests.


\section*{Figures Legends}

Figure should be referred to using a consistent numbering scheme through
the entire Data Descriptor. For initial submissions, authors may choose
to supply this document as a single PDF with embedded figures, but
separate figure image files must be provided for revisions and accepted
manuscripts. In most cases, a Data Descriptor should not contain more
than three figures, but more may be allowed when needed. We discourage
the inclusion of figures in the Supplementary Information \textendash{}
all key figures should be included here in the main Figure section. 

Figure legends begin with a brief title sentence for the whole figure
and continue with a short description of what is shown in each panel,
as well as explaining any symbols used. Legend must total no more
than 350 words, and may contain literature references. 


\section*{Tables}

Tables supporting the Data Descriptor. These can provide summary information
(sample numbers, demographics, etc.), but they should generally not
be used to present primary data (i.e. measurements). Tables containing
primary data should be submitted to an appropriate data repository. 

Tables may be provided within the \LaTeX{} document or as separate
files (tab-delimited text or Excel files). Legends, where needed,
should be included here. Generally, a Data Descriptor should have
fewer than ten Tables, but more may be allowed when needed. Tables
may be of any size, but only Tables which fit onto a single printed
page will be included in the PDF version of the article (up to a maximum
of three). 

\begin{thebibliography}{1}
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\providecommand{\bibinfo}[2]{#2}
\providecommand{\eprint}[2][]{\url{#2}}

\bibitem{cite1}
\bibinfo{author}{Califano, A.}, \bibinfo{author}{Butte, A.~J.},
  \bibinfo{author}{Friend, S.}, \bibinfo{author}{Ideker, T.} \&
  \bibinfo{author}{Schadt, E.}
\newblock \bibinfo{title}{{Leveraging models of cell regulation and GWAS data
  in integrative network-based association studies}}.
\newblock \emph{\bibinfo{journal}{Nature Genetics}}
  \textbf{\bibinfo{volume}{44}}, \bibinfo{pages}{841--847}
  (\bibinfo{year}{2012}).

\bibitem{cite2}
\bibinfo{author}{Wang, R.} \emph{et~al.}
\newblock \bibinfo{title}{{PRIDE Inspector: a tool to visualize and validate MS
  proteomics data.}}
\newblock \emph{\bibinfo{journal}{Nature Biotechnology}}
  \textbf{\bibinfo{volume}{30}}, \bibinfo{pages}{135--137}
  (\bibinfo{year}{2012}).

\end{thebibliography}

\section*{Data Citations}

Bibliographic information for the data records described in the manuscript.

1. Lastname1, Initial1., Lastname2, Initial2., ...\& LastnameN, InitialN. \emph{Repository name} Dataset accession number or DOI (YYYY).

\end{document}

